{"cells":[{"metadata":{"_uuid":"0fdb1f05ab368527aab2a6d69fe4e663de1beff0"},"cell_type":"markdown","source":"  # About Dataset\nThe dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.\n\ntrain.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.\n\ntest.tsv contains just phrases. You must assign a sentiment label to each phrase.\n\nThe sentiment labels are:\n\n0 - negative\n\n1 - somewhat negative\n\n2 - neutral\n\n3 - somewhat positive\n\n4 - positive"},{"metadata":{"_uuid":"6f180c750a0d060cfc94e52c3771e6d67644913f"},"cell_type":"markdown","source":"## Loading important Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\nimport re\nfrom keras.utils import to_categorical\nimport random\nfrom tensorflow import set_random_seed\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Dense,Dropout,Embedding,LSTM\nfrom keras.callbacks import EarlyStopping\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\nlemmatizer = WordNetLemmatizer()\n\n#set random seed for the session and also for tensorflow that runs in background for keras\nset_random_seed(123)\nrandom.seed(123)\n\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f03ae0af5d73a29039f39632d1051ab6de4f3fb"},"cell_type":"markdown","source":"## Load Dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\ntrain= pd.read_csv(\"../input/train.tsv\", sep=\"\\t\")\ntest = pd.read_csv(\"../input/test.tsv\", sep=\"\\t\")\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb1464a9006164fe78aa10d694328dc24159394c"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"732aa6143c98b3c2aaf3a95fe8d3c73920485f53"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47d8b13e01fa17aae32be74d598939403b2cfebb"},"cell_type":"markdown","source":"# Method for cleaning the reviews, tokenize and lemmatize them."},{"metadata":{"trusted":true,"_uuid":"5e1fdc5dbea7aea92a2872c746aea9b25788aad5"},"cell_type":"code","source":"\ndef clean_sentences(df):\n    reviews = []\n\n    for sent in tqdm(df['Phrase']):\n        \n        #remove html content\n        review_text = BeautifulSoup(sent).get_text()\n        \n        #remove non-alphabetic characters\n        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    \n        #tokenize the sentences\n        words = word_tokenize(review_text.lower())\n    \n        #lemmatize each word to its lemma\n        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n    \n        reviews.append(lemma_words)\n\n    return(reviews)\n\n#cleaned reviews for both train and test set retrieved\ntrain_sentences = clean_sentences(train)\ntest_sentences = clean_sentences(test)\nprint(len(train_sentences))\nprint(len(test_sentences))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdedece358c942dbca3baf8e38bb8a850e88b2ba"},"cell_type":"markdown","source":"## Collect the dependent values and convert to one-hot encoded output using to_categorical"},{"metadata":{"trusted":true,"_uuid":"31e462eadcf747d4f37a5e6c2bc4965d39ce10bb"},"cell_type":"code","source":"target=train.Sentiment.values\ny_target=to_categorical(target)\nnum_classes=y_target.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16fbf94281d219307165e3195eb799360cc40a53"},"cell_type":"markdown","source":"## split into train and validation sets."},{"metadata":{"trusted":true,"_uuid":"33db4ea234f1f76a98f2c0dbe7d9b9938a1fe2d3"},"cell_type":"code","source":"\nX_train,X_val,y_train,y_val=train_test_split(train_sentences,y_target,test_size=0.2,stratify=y_target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef359e011decf1ed1f285470d9b3fa1411d55e35"},"cell_type":"markdown","source":"## Geting the no of unique words and max length of a review available in the list of cleaned reviews."},{"metadata":{"trusted":true,"_uuid":"c634680111a84642b151e862040b0475520d5010"},"cell_type":"code","source":" #It is needed for initializing tokenizer of keras and subsequent padding\n\nunique_words = set()\nlen_max = 0\n\nfor sent in tqdm(X_train):\n    \n    unique_words.update(sent)\n    \n    if(len_max<len(sent)):\n        len_max = len(sent)\n        \n#length of the list of unique_words gives the no of unique words\nprint(len(list(unique_words)))\nprint(len_max)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dfc49aeb99251a822a39fed9a8fae0171deb662"},"cell_type":"markdown","source":"## Actual tokenizer of keras and convert to sequences"},{"metadata":{"trusted":true,"_uuid":"6f106b6e59545f8d0d4db272acf32b2337e1ad7b"},"cell_type":"code","source":"\ntokenizer = Tokenizer(num_words=len(list(unique_words)))\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_test = tokenizer.texts_to_sequences(test_sentences)\n\n#padding done to equalize the lengths of all input reviews. LSTM networks needs all inputs to be same length.\n#Therefore reviews lesser than max length will be made equal using extra zeros at end. This is padding.\nX_train = sequence.pad_sequences(X_train, maxlen=len_max)\nX_val = sequence.pad_sequences(X_val, maxlen=len_max)\nX_test = sequence.pad_sequences(X_test, maxlen=len_max)\nprint(X_train.shape,X_val.shape,X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f99132a978d0cfa245defb35edfbfc5e8f6151c6"},"cell_type":"markdown","source":"## Early stopping to prevent overfitting"},{"metadata":{"trusted":true,"_uuid":"27f0a1c26bee93cd660a69df8b79c61094f61abf"},"cell_type":"code","source":"\nearly_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_acc', patience = 2)\ncallback = [early_stopping]\n\n#Model using Keras LSTM\nmodel=Sequential()\nmodel.add(Embedding(len(list(unique_words)),300,input_length=len_max))\nmodel.add(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\nmodel.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\nmodel.add(Dense(100,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d323fed6b9c2ffaeb8b3f71cacb3ff458efe0bf9"},"cell_type":"markdown","source":"## fit the model"},{"metadata":{"trusted":true,"_uuid":"2b1bf946e278f0a9955356351907d512f5018582"},"cell_type":"code","source":"\n#This is done for learning purpose only. One can play around with different hyper parameters combinations\n#and try increase the accuracy even more. For example, a different learning rate, an extra dense layer \n# before output layer, etc. Cross validation could be used to evaluate the model and grid search \n# further to find unique combination of parameters that give maximum accuracy. This model has a validation\n#accuracy of around 66.5%\nhistory=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=6, batch_size=256, verbose=1, callbacks=callback)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1524533e318bcdea0e4d3abebbf5b02c6eb49b2b"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Create count of the number of epochs\nepoch_count = range(1, len(history.history['loss']) + 1)\n\n# Visualize learning curve. Here learning curve is not ideal. It should be much smoother as it decreases.\n#As mentioned before, altering different hyper parameters especially learning rate can have a positive impact\n#on accuracy and learning curve.\nplt.plot(epoch_count, history.history['loss'], 'r--')\nplt.plot(epoch_count, history.history['val_loss'], 'b-')\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9356081721990b2af9de625c7fcad7c241ee61a"},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true,"_uuid":"03b12920b45234667f67adeedb5596e93a4d3ddd"},"cell_type":"code","source":"#make the predictions with trained model and submit the predictions.\ny_pred=model.predict_classes(X_test)\n\nsub_file = pd.read_csv('../input/sampleSubmission.csv',sep=',')\nsub_file.Sentiment=y_pred\nsub_file.to_csv('Submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfc1e7a845c15007bfeeae2e3843c33bdfa5f160"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f0f66e24852c120c8ac809475f58fe4e87f1d9d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}